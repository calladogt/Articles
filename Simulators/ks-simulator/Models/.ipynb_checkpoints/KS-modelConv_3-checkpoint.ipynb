{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEmv9FsyCKU6"
   },
   "source": [
    "# Prediction of the KS system: a model using Convolutional Neural Network\n",
    "\n",
    "In this notebook, I will use the CNN to predict many step of the KS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rrWwDC5TCKU9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import time\n",
    "from scipy import stats\n",
    "#from KS import KS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vblhn23zCZHj"
   },
   "outputs": [],
   "source": [
    "#Set a javascript interval to click on the connect button every 60 seconds. \n",
    "#Open developer-settings (in your web-browser) with Ctrl+Shift+i then click \n",
    "#on console tab and type this on the console prompt.\n",
    "function ClickConnect(){\n",
    "    console.log(\"Clicked on connect button\"); \n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "seMr-weECKVF"
   },
   "outputs": [],
   "source": [
    "def drawKS(U_sim, x_axis, t_axis ,startT=0,endT=0, width = 10, divwidth = 4): \n",
    "    \"\"\"\n",
    "    This function will plot the evolution of the KS system (U_sim) over time (t_axis)\n",
    "    \n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(width, width/divwidth))\n",
    "    ax = fig.gca()\n",
    "    ola = ax.contourf(t_axis[startT:endT], x_axis, U_sim[startT:endT,:].T, 15)\n",
    "    cbar = plt.colorbar(ola)\n",
    "    plt.xlabel('Time evolution')\n",
    "    plt.ylabel('Position X')\n",
    "    plt.title('Spatiotemporal solution of KS equation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s7_tBZVPCKVJ"
   },
   "outputs": [],
   "source": [
    "def plothist(state_history,bins=30):\n",
    "    \"\"\"\n",
    "    plot the histogram of KS system data with the correct label\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    plt.hist(state_history,bins=30)\n",
    "    plt.xlabel('Values of velocity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of state_history')\n",
    "    plt.grid(True, color=\"#93a1a1\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h6x5PBNqCKVN"
   },
   "outputs": [],
   "source": [
    "def statvalues(state_history):\n",
    "    \"\"\"\n",
    "    print some statistics values of the KS data\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Mean:', np.mean(state_history))\n",
    "    print('Median:', np.median(state_history))\n",
    "    print('Max:', np.max(state_history))\n",
    "    print('Min:', np.min(state_history))\n",
    "    print('Mode:', stats.mode(state_history))\n",
    "    print('Variance:', np.var(state_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "miDKdnW2CKVQ"
   },
   "outputs": [],
   "source": [
    "def evolutionstat(state_history):\n",
    "  \"\"\"\n",
    "  plot the temporal evolution of some statistics values\n",
    "  \n",
    "  \"\"\"  \n",
    "  plt.subplots(figsize=(15, 7))\n",
    "  plt.subplot(2,3,1)\n",
    "  plt.plot(np.mean(state_history,axis=1))\n",
    "  plt.title('Mean')\n",
    "  plt.ylabel('Mean value')\n",
    "  plt.subplot(2,3,2)\n",
    "  plt.plot(np.median(state_history,axis=1))\n",
    "  plt.title('Median')\n",
    "  plt.ylabel('Median value')\n",
    "  plt.subplot(2,3,3)\n",
    "  plt.plot(np.max(state_history,axis=1))\n",
    "  plt.title('Max')\n",
    "  plt.ylabel('Max value')\n",
    "  plt.xlabel('Time evolution')\n",
    "  plt.subplot(2,3,4)\n",
    "  plt.plot(np.min(state_history,axis=1))\n",
    "  plt.title('Min')\n",
    "  plt.ylabel('Min value')\n",
    "  plt.xlabel('Time evolution')\n",
    "  plt.subplot(2,3,5)\n",
    "  plt.plot(np.var(state_history,axis=1))\n",
    "  plt.title('Variance')\n",
    "  plt.ylabel('Variance value')\n",
    "  plt.xlabel('Time evolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q8ft6IKHCvVa"
   },
   "outputs": [],
   "source": [
    "def prediction(model,start_position,N_step_simu,steps_to_predict,size_conv_time,state_history_norm_th):\n",
    "    \"\"\"\n",
    "    This function will simulate the model for a given start_position of \n",
    "    state_history_norm_th from start_position to start_position+N_step_pred\n",
    "    and compares this simulation with the true value given by state_history_norm_th\n",
    "    \n",
    "    input:\n",
    "    - model: the model that will be used to make the prediction\n",
    "    - start_position: the position in the vector state_history_norm_th that will start the simulation\n",
    "    - N_step_pred: number of the steps to simulate\n",
    "    - state_history_norm_th: the true simulation given by KS.py. it will be used as the start point\n",
    "      to initiate our model simulation and to verify the quality of this simulation\n",
    "    \n",
    "    \n",
    "    output:\n",
    "    - print: the true spatiotemporal evolution of KS system\n",
    "             our simulation by the model\n",
    "             the difference between them per step of the simulation\n",
    "    \n",
    "    state_history: all the spatialtemporal evolution of the system. shape: [number of iteractions,samples_points]\n",
    "    time_counter: array of the time corresponding to each interaction\n",
    "    position_x: array of samples_points points that goes from 0 to length\n",
    "    \"\"\"\n",
    "\n",
    "    y_true=state_history_norm_th[start_position:start_position+(N_step_simu*steps_to_predict),:].squeeze()\n",
    "    sample_points=state_history_norm_th.size(1)\n",
    "    current_state = th.zeros((1, sample_points, size_conv_time))\n",
    "    current_state[0,:,:] = state_history_norm_th[start_position-(size_conv_time):start_position,:].T\n",
    "    current_state=current_state.double().cuda()\n",
    "    prediction_model = th.zeros((N_step_simu, steps_to_predict,sample_points))\n",
    "\n",
    "    for i in range(N_step_simu):\n",
    "        y=model(current_state)\n",
    "        yy=y[0].reshape(sample_points,steps_to_predict)\n",
    "        yy=yy.T\n",
    "        prediction_model[i,:,:]=yy\n",
    "        y=y.reshape(1,sample_points,steps_to_predict)\n",
    "        past=current_state[:,:,steps_to_predict:size_conv_time]\n",
    "        current_state=th.zeros(1,sample_points,size_conv_time).double().cuda()\n",
    "        current_state[:,:,0:(size_conv_time-steps_to_predict)]=past\n",
    "        current_state[:,:,(size_conv_time-steps_to_predict):]=y\n",
    "\n",
    "\n",
    "    prediction_model_np = prediction_model.detach().numpy()\n",
    "    prediction_model_np= prediction_model_np.reshape(N_step_simu*steps_to_predict,sample_points)\n",
    "    y_true_np = y_true.cpu().numpy()\n",
    "    dif=np.mean(np.absolute(prediction_model_np-y_true_np),axis=1)\n",
    "\n",
    "\n",
    "    print('The true solution:')\n",
    "    drawKS(y_true_np, position_x, time_counter[start_position:start_position+N_step_simu*steps_to_predict] ,startT=1,endT=N_step_simu*steps_to_predict, width = 10, divwidth = 4) \n",
    "    print('The model solution:')\n",
    "    drawKS(prediction_model_np, position_x,time_counter[start_position:start_position+N_step_simu*steps_to_predict] ,startT=1,endT=N_step_simu*steps_to_predict, width = 10, divwidth = 4)\n",
    "\n",
    "    plt.plot(np.absolute(dif))\n",
    "    plt.title('Difference between prediction and true value')\n",
    "    plt.ylabel('Delta')\n",
    "    plt.xlabel('Time step')\n",
    "    \n",
    "    return prediction_model_np,y_true_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcwP8Uh-CKVU"
   },
   "source": [
    "## Loading and normalizing the data\n",
    "First, let's set the parameters of the simulation. Pay attention to set the correct parameters to the data set loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qi00m-dVCKVV"
   },
   "outputs": [],
   "source": [
    "final_time = 10000 # the total time of the simulation\n",
    "dt = 0.05 # the time step \n",
    "length = 22 # the \"physical\" length of the domaine. pay attention cuz this value will determine if the system is regular or chaotic\n",
    "sample_points = 64 # the number of sample points on this length. It will be the size of the input in our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../kaggle/input/data-of-the-simulation-u3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13027,
     "status": "ok",
     "timestamp": 1591734657656,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "dm-yUw0WCKVi",
    "outputId": "6854ab10-c35b-4246-8312-02d27e0e9d6b"
   },
   "outputs": [],
   "source": [
    "#to load data\n",
    "\n",
    "from numpy import load\n",
    "# load numpy array\n",
    "state_history = np.loadtxt('state_history_u3.dat')\n",
    "time_counter = np.loadtxt('time_counter_u3.dat')\n",
    "position_x = np.loadtxt('position_u3.dat')\n",
    "\n",
    "print('The state_history shape is:', state_history.shape)\n",
    "print('The final time:', time_counter[-1])\n",
    "print('Total length', position_x[1]+position_x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn5WELW7CKVv"
   },
   "source": [
    "Let's normalize the data to be between [-0.975,0.975]. It will allow us to use other activation fonction (like Tanh):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7042,
     "status": "ok",
     "timestamp": 1591739946561,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "zIIeDJseCKVw",
    "outputId": "8260bcc9-7079-418f-d74b-13ae58ecbff0"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-0.975,0.975))\n",
    "state_history_norm = min_max_scaler.fit_transform(state_history)\n",
    "\n",
    "#histogram\n",
    "plothist(state_history_norm,bins=30)\n",
    "\n",
    "#statistic values\n",
    "statvalues(state_history_norm[0,:])\n",
    "\n",
    "#evolution of statistic values\n",
    "evolutionstat(state_history_norm)\n",
    "\n",
    "#KSflow\n",
    "drawKS(state_history_norm, position_x, time_counter ,startT=1,endT=int(time_counter[-1]), width = 10, divwidth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaAcatrRCKVz"
   },
   "source": [
    "## Separating the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1591734693334,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "d0h5De2KDJei",
    "outputId": "723a1f36-0a3e-44c1-ee24-3f650fa3463e"
   },
   "outputs": [],
   "source": [
    "state_history_norm=state_history_norm[:100000,:]\n",
    "#até 100.000 vai, com 200.000 o colabs nao aguenta\n",
    "state_history_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLWUBz-aCKV0"
   },
   "outputs": [],
   "source": [
    "size_conv_time=150\n",
    "steps_to_predict=50\n",
    "\n",
    "X = state_history_norm[:-size_conv_time,:] #our input: all the data excluding the last size_conv_time data\n",
    "Y = state_history_norm[size_conv_time:,:] #our output: all the data minus the first size_conv_time one\n",
    "\n",
    "NN = state_history_norm.shape[0]-2*size_conv_time\n",
    "Nt = int(NN*0.9) # number of train\n",
    "Nv = int(NN*0.1) # number of validation\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "indices=np.arange(NN)\n",
    "np.random.shuffle(indices)\n",
    "train_ind=indices[0:Nt]\n",
    "valid_ind=indices[-Nv:]\n",
    "\n",
    "\n",
    "X_train = np.zeros((Nt, sample_points, size_conv_time))\n",
    "aux=0\n",
    "for ind in train_ind:\n",
    "    X_train[aux,:,:]=X[ind:ind+size_conv_time].T\n",
    "    aux=aux+1\n",
    "\n",
    "Y_train = np.zeros((Nt, sample_points, steps_to_predict))\n",
    "aux=0\n",
    "for ind in train_ind:\n",
    "    Y_train[aux,:,:]=Y[ind:ind+steps_to_predict].T\n",
    "    aux=aux+1\n",
    "\n",
    "Y_train = Y_train.reshape(Nt,sample_points*steps_to_predict)\n",
    "\n",
    "X_valid = np.zeros((Nv, sample_points, size_conv_time))\n",
    "aux=0\n",
    "for ind in valid_ind:\n",
    "    X_valid[aux,:,:]=X[ind:ind+size_conv_time].T\n",
    "    aux=aux+1\n",
    "\n",
    "Y_valid = np.zeros((Nv, sample_points, steps_to_predict))\n",
    "aux=0\n",
    "for ind in valid_ind:\n",
    "    Y_valid[aux,:,:]=Y[ind:ind+steps_to_predict].T\n",
    "    aux=aux+1\n",
    "\n",
    "Y_valid = Y_valid.reshape(Nv,sample_points*steps_to_predict)\n",
    "\n",
    "#converting to tensor\n",
    "X_train_th = th.from_numpy(X_train).cuda()\n",
    "Y_train_th = th.from_numpy(Y_train).cuda()\n",
    "X_valid_th = th.from_numpy(X_valid).cuda()\n",
    "Y_valid_th = th.from_numpy(Y_valid).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1591388993804,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "ShbN87s3CKV3",
    "outputId": "3d68b134-cb60-4b30-b2ee-dab774fb7b9d"
   },
   "outputs": [],
   "source": [
    "print(X_train_th.shape)\n",
    "print(Y_train_th.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW-FIkBUCKV9"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei7CYP-UCKV-"
   },
   "outputs": [],
   "source": [
    "def train(model,X_train,X_valid,Y_train,Y_valid,epochs=1000,batch_size=64):\n",
    "    Nt=len(X_train)\n",
    "    Nv=len(X_valid)\n",
    "    ### Mini-batching and shuffle\n",
    "    idx = np.arange(Nt)\n",
    "    nbatch = int(Nt/batch_size)\n",
    "    train_losses,valid_losses=[],[]\n",
    "    start_time=time.time()\n",
    "    print(\"Epoch\\tTrain Loss\\tValid Loss\")\n",
    "    \n",
    "    #Just to see the prediction of our random parameters\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        running_loss=0\n",
    "        for bi in range(nbatch):\n",
    "            ids = idx[bi*batch_size:(bi+1)*batch_size]\n",
    "            x = X_train[ids]\n",
    "            ttf_calc=model(x).squeeze()\n",
    "            ttf = Y_train[ids]\n",
    "            loss=loss_fn(ttf_calc,ttf)\n",
    "            running_loss+=loss.item()\n",
    "        train_losses.append(running_loss/nbatch)\n",
    "        running_loss=0\n",
    "        ttf_calc=model(X_valid).squeeze()\n",
    "        ttf = Y_valid\n",
    "        loss=loss_fn(ttf_calc,ttf)\n",
    "        running_loss+=loss.item()\n",
    "        valid_losses.append(running_loss)\n",
    "        #it will print the result in epoch=0 during the training\n",
    "    \n",
    "    #starting the training here:\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        #print(\"{}\\t\".format(e),\"{:.5f}..\\t\".format(train_losses[-1]),\"{:.5f}..\".format(valid_losses[-1]))  \n",
    "        np.random.shuffle(idx)\n",
    "        running_loss=0\n",
    "        for bi in range(nbatch):\n",
    "            ids = idx[bi*batch_size:(bi+1)*batch_size]\n",
    "            x = X_train[ids]\n",
    "            optimizer.zero_grad()\n",
    "            ttf_calc=model(x).squeeze()\n",
    "            ttf = Y_train[ids]\n",
    "            loss=loss_fn(ttf_calc,ttf)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "        # training is over for one epoch\n",
    "        #now evaluate the model with the valid set:\n",
    "        model.eval()\n",
    "        accuracy=0\n",
    "        train_losses.append(running_loss/nbatch)\n",
    "        running_loss=0\n",
    "        with th.no_grad():\n",
    "            x = X_valid\n",
    "            ttf_calc=model(x).squeeze()\n",
    "            ttf = Y_valid\n",
    "            loss=loss_fn(ttf_calc,ttf)\n",
    "            running_loss+=loss.item()\n",
    "            valid_losses.append(running_loss)\n",
    "    #print(\"{}\\t\".format(e+1),\"{:.5f}..\\t\".format(train_losses[-1]),\"{:.5f}..\".format(valid_losses[-1]))     \n",
    "    #print(\"---------- Best : {:.3f}\".format(min(valid_losses)), \" at epoch \" \n",
    "    #      , np.fromiter(valid_losses, dtype=np.float).argmin(), \" / \",epochs )\n",
    "    #print('Execution time={:.2f}s'.format(time.time()-start_time))\n",
    "    #plot some graphs\n",
    "    #plt.figure(figsize = (9, 1.5))\n",
    "    #ax= plt.subplot(1, 2, 1)\n",
    "    #plt.plot(train_losses)\n",
    "    #ax.set_ylim(bottom=0)\n",
    "    #ax= plt.subplot(1, 2, 2)\n",
    "    #plt.plot(valid_losses)\n",
    "    #ax.set_ylim(bottom=0)\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7RJWJgWCKWI"
   },
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mLhDnJ9zCKWJ"
   },
   "outputs": [],
   "source": [
    "Lin = 100\n",
    "in_ch_conv_1 = 64\n",
    "out_ch_conv_1 = 128\n",
    "k_conv_1 = 10\n",
    "st_conv_1 = 2\n",
    "pad_conv_1 = 4\n",
    "L_2 = (Lin+2*pad_conv_1-(k_conv_1-1)-1)/st_conv_1+1\n",
    "\n",
    "k_pool_1= 2\n",
    "st_pool_1= 2\n",
    "L_3 = (L_2 - k_pool_1)/st_pool_1 + 1\n",
    "\n",
    "in_ch_conv_2 = 128\n",
    "out_ch_conv_2 = 64\n",
    "k_conv_2 = 5\n",
    "st_conv_2 = 2\n",
    "pad_conv_2 = 1\n",
    "L_4 = (L_3+2*pad_conv_2-(k_conv_2-1)-1)/st_conv_2+1\n",
    "\n",
    "k_pool_2= 12\n",
    "st_pool_2= 1\n",
    "L_5 = (L_4 - k_pool_2)/st_pool_2 + 1\n",
    "\n",
    "in_lin = sample_points\n",
    "hidden_1 = sample_points*5\n",
    "out_lin = sample_points*10\n",
    "\n",
    "\n",
    "model = th.nn.Sequential(\n",
    "    th.nn.Conv1d(in_ch_conv_1,out_ch_conv_1,k_conv_1,st_conv_1,pad_conv_1),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.MaxPool1d(k_pool_1,st_pool_1),\n",
    "    th.nn.Conv1d(in_ch_conv_2,out_ch_conv_2,k_conv_2,st_conv_2,pad_conv_2),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.MaxPool1d(k_pool_2,st_pool_2),\n",
    "    th.nn.Flatten(),\n",
    "    th.nn.Linear(in_lin,hidden_1),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.Linear(hidden_1,out_lin),\n",
    "    th.nn.Tanh(),\n",
    ")\n",
    "model.double()\n",
    "model.cuda()\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=1e-5)\n",
    "loss_fn=th.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1668286,
     "status": "ok",
     "timestamp": 1591742639192,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "uJ6l9SUVCKWM",
    "outputId": "837bf1ab-9462-4cdc-da60-de7b70e32720"
   },
   "outputs": [],
   "source": [
    "epochs_counter=100\n",
    "batch_size=64\n",
    "train_losses, valid_losses = train(model,X_train_th,X_valid_th,Y_train_th,Y_valid_th,epochs=epochs_counter,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "glc7JcBEEBC0"
   },
   "outputs": [],
   "source": [
    "#TO SAVE THE MODEL\n",
    "\n",
    "tosave_data = {'model': model,\n",
    "               'epoch': epoch,\n",
    "               'batch_size': batch_size,\n",
    "               'model_state_dict': model.state_dict(),\n",
    "               'optimizer_state_dict': optimizer.state_dict(),\n",
    "               'loss': loss_fn,\n",
    "               'training loss': train_losses,\n",
    "               'valid loss': valid_losses,}\n",
    "# Write a pickle file using pytorch \n",
    "th.save(tosave_data, \"modelcv3_model2_u3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3387,
     "status": "ok",
     "timestamp": 1591742844837,
     "user": {
      "displayName": "Gabriel Callado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghnvi183GuMBSeeGPSUTAterZ8HcEuC0_0pR7xFtA=s64",
      "userId": "08786233999723143686"
     },
     "user_tz": 180
    },
    "id": "wWXFXfBNEJr6",
    "outputId": "793db3d0-be66-4a79-fc29-46cec2069ce7"
   },
   "outputs": [],
   "source": [
    "state_history_norm_th = th.from_numpy(state_history_norm).cuda()\n",
    "#state_history_norm_th = state_history_norm_th.unsqueeze(0)\n",
    "start_position=200\n",
    "N_step_simu=4\n",
    "print('Start_position=', start_position)\n",
    "pp,yy = prediction(model,start_position,N_step_simu,steps_to_predict,size_conv_time,state_history_norm_th)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin = 150\n",
    "in_ch_conv_1 = 64\n",
    "out_ch_conv_1 = 128\n",
    "k_conv_1 = 4\n",
    "st_conv_1 = 2\n",
    "pad_conv_1 = 4\n",
    "L_2 = (Lin+2*pad_conv_1-(k_conv_1-1)-1)/st_conv_1+1\n",
    "\n",
    "k_pool_1= 2\n",
    "st_pool_1= 2\n",
    "L_3 = (L_2 - k_pool_1)/st_pool_1 + 1\n",
    "\n",
    "in_ch_conv_2 = 128\n",
    "out_ch_conv_2 = 64\n",
    "k_conv_2 = 5\n",
    "st_conv_2 = 2\n",
    "pad_conv_2 = 1\n",
    "L_4 = (L_3+2*pad_conv_2-(k_conv_2-1)-1)/st_conv_2+1\n",
    "\n",
    "k_pool_2= 5\n",
    "st_pool_2= 2\n",
    "L_5 = (L_4 - k_pool_2)/st_pool_2 + 1\n",
    "\n",
    "in_lin = int(sample_points*L_5)\n",
    "hidden_1 = sample_points*25\n",
    "out_lin = sample_points*50\n",
    "\n",
    "model = th.nn.Sequential(\n",
    "    th.nn.Conv1d(in_ch_conv_1,out_ch_conv_1,k_conv_1,st_conv_1,pad_conv_1),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.MaxPool1d(k_pool_1,st_pool_1),\n",
    "    th.nn.Conv1d(in_ch_conv_2,out_ch_conv_2,k_conv_2,st_conv_2,pad_conv_2),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.MaxPool1d(k_pool_2,st_pool_2),\n",
    "    th.nn.Flatten(),\n",
    "    th.nn.Linear(in_lin,hidden_1),\n",
    "    th.nn.LeakyReLU(),\n",
    "    th.nn.Linear(hidden_1,out_lin),\n",
    "    th.nn.Tanh(),\n",
    ")\n",
    "model.double()\n",
    "model.cuda()\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=1e-5)\n",
    "loss_fn=th.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_counter=500\n",
    "batch_size=64\n",
    "train_losses, valid_losses = train(model,X_train_th,X_valid_th,Y_train_th,Y_valid_th,epochs=epochs_counter,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_history_norm_th = th.from_numpy(state_history_norm).cuda()\n",
    "#state_history_norm_th = state_history_norm_th.unsqueeze(0)\n",
    "start_position=200\n",
    "N_step_simu=100\n",
    "print('Start_position=', start_position)\n",
    "pp,yy = prediction(model,start_position,N_step_simu,steps_to_predict,size_conv_time,state_history_norm_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_history_norm_th = th.from_numpy(state_history_norm).cuda()\n",
    "#state_history_norm_th = state_history_norm_th.unsqueeze(0)\n",
    "start_position=3000\n",
    "N_step_simu=30\n",
    "print('Start_position=', start_position)\n",
    "pp,yy = prediction(model,start_position,N_step_simu,steps_to_predict,size_conv_time,state_history_norm_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    epochs_counter=5\n",
    "    batch_size=64\n",
    "    train_losses, valid_losses = train(model,X_train_th,X_valid_th,Y_train_th,Y_valid_th,epochs=epochs_counter,batch_size=batch_size)\n",
    "    tosave_data = {'model': model,\n",
    "               'epoch': epochs_counter,\n",
    "               'batch_size': batch_size,\n",
    "               'model_state_dict': model.state_dict(),\n",
    "               'optimizer_state_dict': optimizer.state_dict(),\n",
    "               'loss': loss_fn,\n",
    "               'training loss': train_losses,\n",
    "               'valid loss': valid_losses,}\n",
    "    th.save(tosave_data, str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with different steps_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    \n",
    "    size_conv_time=150\n",
    "    steps_to_predict = (i+1)*5\n",
    "\n",
    "    Y_train = np.zeros((Nt, sample_points, steps_to_predict))\n",
    "    aux=0\n",
    "    for ind in train_ind:\n",
    "        Y_train[aux,:,:]=Y[ind:ind+steps_to_predict].T\n",
    "        aux=aux+1\n",
    "\n",
    "    Y_train = Y_train.reshape(Nt,sample_points*steps_to_predict)\n",
    "\n",
    "\n",
    "    Y_valid = np.zeros((Nv, sample_points, steps_to_predict))\n",
    "    aux=0\n",
    "    for ind in valid_ind:\n",
    "        Y_valid[aux,:,:]=Y[ind:ind+steps_to_predict].T\n",
    "        aux=aux+1\n",
    "\n",
    "    Y_valid = Y_valid.reshape(Nv,sample_points*steps_to_predict)\n",
    "\n",
    "    #converting to tensor\n",
    "    Y_train_th = th.from_numpy(Y_train).cuda()\n",
    "    Y_valid_th = th.from_numpy(Y_valid).cuda()\n",
    "    \n",
    "    Lin = size_conv_time\n",
    "    in_ch_conv_1 = 64\n",
    "    out_ch_conv_1 = 128\n",
    "    k_conv_1 = 4\n",
    "    st_conv_1 = 2\n",
    "    pad_conv_1 = 4\n",
    "    L_2 = (Lin+2*pad_conv_1-(k_conv_1-1)-1)/st_conv_1+1\n",
    "\n",
    "    k_pool_1= 2\n",
    "    st_pool_1= 2\n",
    "    L_3 = (L_2 - k_pool_1)/st_pool_1 + 1\n",
    "\n",
    "    in_ch_conv_2 = 128\n",
    "    out_ch_conv_2 = 64\n",
    "    k_conv_2 = 5\n",
    "    st_conv_2 = 2\n",
    "    pad_conv_2 = 1\n",
    "    L_4 = (L_3+2*pad_conv_2-(k_conv_2-1)-1)/st_conv_2+1\n",
    "\n",
    "    k_pool_2= 5\n",
    "    st_pool_2= 2\n",
    "    L_5 = (L_4 - k_pool_2)/st_pool_2 + 1\n",
    "\n",
    "    in_lin = int(sample_points*L_5)\n",
    "    hidden_1 = sample_points*25\n",
    "    out_lin = int(sample_points*steps_to_predict)\n",
    "\n",
    "    model = th.nn.Sequential(\n",
    "        th.nn.Conv1d(in_ch_conv_1,out_ch_conv_1,k_conv_1,st_conv_1,pad_conv_1),\n",
    "        th.nn.LeakyReLU(),\n",
    "        th.nn.MaxPool1d(k_pool_1,st_pool_1),\n",
    "        th.nn.Conv1d(in_ch_conv_2,out_ch_conv_2,k_conv_2,st_conv_2,pad_conv_2),\n",
    "        th.nn.LeakyReLU(),\n",
    "        th.nn.MaxPool1d(k_pool_2,st_pool_2),\n",
    "        th.nn.Flatten(),\n",
    "        th.nn.Linear(in_lin,hidden_1),\n",
    "        th.nn.LeakyReLU(),\n",
    "        th.nn.Linear(hidden_1,out_lin),\n",
    "        th.nn.Tanh(),\n",
    "    )\n",
    "    model.double()\n",
    "    model.cuda()\n",
    "    optimizer=th.optim.Adam(model.parameters(),lr=1e-5)\n",
    "    loss_fn=th.nn.MSELoss()\n",
    "    \n",
    "    epochs_counter=100\n",
    "    batch_size=64\n",
    "    train_losses, valid_losses = train(model,X_train_th,X_valid_th,Y_train_th,Y_valid_th,epochs=epochs_counter,batch_size=batch_size)\n",
    "    tosave_data = {'epoch': epochs_counter,\n",
    "               'batch_size': batch_size,\n",
    "               'model_state_dict': model.state_dict(),\n",
    "               'optimizer_state_dict': optimizer.state_dict(),\n",
    "               'loss': loss_fn,\n",
    "               'training loss': train_losses,\n",
    "               'valid loss': valid_losses,}\n",
    "    th.save(tosave_data, str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
